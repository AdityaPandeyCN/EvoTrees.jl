# initialize train_nodes
function grow_tree(Œ¥, Œ¥¬≤, ùë§,
    hist_Œ¥, hist_Œ¥¬≤, hist_ùë§,
    params::Union{EvoTreeRegressor,EvoTreeCount,EvoTreeClassifier,EvoTreeGaussian},
    train_nodes::Vector{TrainNode{L,T,S}},
    splits::Vector{SplitInfo{L,T,Int}},
    edges, X_bin) where {R<:Real, T<:AbstractFloat, S<:Int, L}

    active_id = ones(Int, 1)
    leaf_count = one(Int)
    tree_depth = one(Int)
    tree = Tree(Vector{TreeNode{params.K, T, Int, Bool}}())

    # grow while there are remaining active nodes
    while size(active_id, 1) > 0 && tree_depth <= params.max_depth
        next_active_id = ones(Int, 0)
        # grow nodes
        for id in active_id
            node = train_nodes[id]
            if tree_depth == params.max_depth || node.‚àëùë§[1] <= params.min_weight
                push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, Œ¥¬≤)))
            else
                @threads for feat in node.ùëó
                    splits[feat].gain = node.gain
                    find_split_static!(hist_Œ¥[feat], hist_Œ¥¬≤[feat], hist_ùë§[feat], view(X_bin,:,feat), Œ¥, Œ¥¬≤, ùë§, node.‚àëŒ¥, node.‚àëŒ¥¬≤, node.‚àëùë§, params, splits[feat], edges[feat], node.ùëñ)
                    # update_hist!(hist_Œ¥, hist_Œ¥¬≤, hist_ùë§, X_bin, Œ¥, Œ¥¬≤, ùë§, set, feat)
                    # find_split!(hist_Œ¥[feat], hist_Œ¥¬≤[feat], hist_ùë§[feat], node.‚àëŒ¥, node.‚àëŒ¥¬≤, node.‚àëùë§, params, splits[feat], edges[feat], feat)
                end
                # assign best split
                best = get_max_gain(splits)
                # grow node if best split improve gain
                if best.gain > node.gain + params.Œ≥
                    # Node: depth, ‚àëŒ¥, ‚àëŒ¥¬≤, gain, ùëñ, ùëó
                    # set = BitSet(node.ùëñ)
                    left, right = update_set(node.ùëñ, best.ùëñ, view(X_bin,:,best.feat))
                    train_nodes[leaf_count + 1] = TrainNode(node.depth + 1, best.‚àëŒ¥L, best.‚àëŒ¥¬≤L, best.‚àëùë§L, best.gainL, left, node.ùëó)
                    train_nodes[leaf_count + 2] = TrainNode(node.depth + 1, best.‚àëŒ¥R, best.‚àëŒ¥¬≤R, best.‚àëùë§R, best.gainR, right, node.ùëó)
                    # push split Node
                    push!(tree.nodes, TreeNode(leaf_count + 1, leaf_count + 2, best.feat, best.cond, params.K))
                    push!(next_active_id, leaf_count + 1)
                    push!(next_active_id, leaf_count + 2)
                    leaf_count += 2
                else
                    push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, Œ¥¬≤)))
                end # end of single node split search
            end
        end # end of loop over active ids for a given depth
        active_id = next_active_id
        tree_depth += 1
    end # end of tree growth
    return tree
end

# extract the gain value from the vector of best splits and return the split info associated with best split
function get_max_gain(splits::Vector{SplitInfo{L,T,S}}) where {L,T,S}
    gains = (x -> x.gain).(splits)
    feat = findmax(gains)[2]
    best = splits[feat]
    return best
end

# grow_gbtree
function grow_gbtree(X::AbstractArray{R, 2}, Y::AbstractVector{S}, params::Union{EvoTreeRegressor,EvoTreeCount,EvoTreeClassifier,EvoTreeGaussian};
    X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractVector{S} = Vector{S}(undef, 0),
    early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, S<:Real}

    seed!(params.seed)

    Œº = ones(params.K)
    Œº .*= mean(Y)
    if typeof(params.loss) == Logistic
        Œº .= logit.(Œº)
    elseif typeof(params.loss) == Poisson
        Œº .= log.(Œº)
    elseif typeof(params.loss) == Softmax
        Œº .*= 0.0
    elseif typeof(params.loss) == Gaussian
        Œº = SVector{2}([mean(Y), log(var(Y))])
    end

    # initialize preds
    pred = zeros(SVector{params.K,Float64}, size(X,1))
    for i in eachindex(pred)
        pred[i] += Œº
    end

    # eval init
    if size(Y_eval, 1) > 0
        # pred_eval = ones(size(Y_eval, 1), params.K) .* Œº'
        pred_eval = zeros(SVector{params.K,Float64}, size(X_eval,1))
        for i in eachindex(pred_eval)
            pred_eval[i] += Œº
        end
    end

    # bias = Tree([TreeNode(SVector{1, Float64}(Œº))])
    bias = Tree([TreeNode(SVector{params.K,Float64}(Œº))])
    gbtree = GBTree([bias], params, Metric())

    X_size = size(X)
    ùëñ_ = collect(1:X_size[1])
    ùëó_ = collect(1:X_size[2])

    # initialize gradients and weights
    Œ¥ = zeros(SVector{params.K, Float64}, X_size[1])
    Œ¥¬≤ = zeros(SVector{params.K, Float64}, X_size[1])
    ùë§ = zeros(SVector{1, Float64}, X_size[1])
    for i in eachindex(ùë§)
        ùë§[i] += ones(1)
    end

    edges = get_edges(X, params.nbins)
    X_bin = binarize(X, edges)

    # initialize train nodes
    train_nodes = Vector{TrainNode{params.K, Float64, Int64}}(undef, 2^params.max_depth-1)
    for node in 1:2^params.max_depth-1
        train_nodes[node] = TrainNode(0, SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{1, Float64}(fill(-Inf, 1)), -Inf, [0], [0])
    end

    # initializde node splits info and tracks - colsample size (ùëó)
    splits = Vector{SplitInfo{params.K, Float64, Int64}}(undef, X_size[2])
    hist_Œ¥ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_Œ¥¬≤ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_ùë§ = Vector{Vector{SVector{1, Float64}}}(undef, X_size[2])
    for feat in ùëó_
        splits[feat] = SplitInfo{params.K, Float64, Int}(-Inf, SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), -Inf, -Inf, 0, feat, 0.0)
        hist_Œ¥[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_Œ¥¬≤[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_ùë§[feat] = zeros(SVector{1, Float64}, length(edges[feat]))
    end

    # initialize metric
    if params.metric != :none
        metric_track = Metric()
        metric_best = Metric()
        iter_since_best = 0
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ùëñ = ùëñ_[sample(ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace=false, ordered=true)]
        ùëó = ùëó_[sample(ùëó_, ceil(Int, params.colsample * X_size[2]), replace=false, ordered=true)]

        # reset gain to -Inf
        for feat in ùëó_
            splits[feat].gain = -Inf
        end

        # get gradients
        update_grads!(params.loss, params.Œ±, pred, Y, Œ¥, Œ¥¬≤, ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(Œ¥[ùëñ]), sum(Œ¥¬≤[ùëñ]), sum(ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(Œ¥, Œ¥¬≤, ùë§, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§, params, train_nodes, splits, edges, X_bin)
        # push new tree to model
        push!(gbtree.trees, tree)
        # update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end

        # callback function
        if params.metric != :none
            if size(Y_eval, 1) > 0
                metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Œ±)
            else
                metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Œ±)
            end

            if metric_track.metric < metric_best.metric
                metric_best.metric .=  metric_track.metric
                metric_best.iter .=  i
            else
                iter_since_best += 1
            end

            if mod(i, print_every_n) == 0 && verbosity > 0
                display(string("iter:", i, ", eval: ", metric_track.metric))
            end
            iter_since_best >= early_stopping_rounds ? break : nothing
        end # end of callback

    end #end of nrounds

    if params.metric != :none
        gbtree.metric.iter .= metric_best.iter
        gbtree.metric.metric .= metric_best.metric
    end
    return gbtree
end

# grow_gbtree - continue training
function grow_gbtree!(model::GBTree, X::AbstractArray{R, 2}, Y::AbstractVector{S};
    X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractVector{S} = Vector{S}(undef, 0),
    early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, S<:Real}

    params = model.params
    seed!(params.seed)

    # initialize predictions - efficiency to be improved
    pred = zeros(SVector{params.K,Float64}, size(X,1))
    pred_ = predict(model, X)
    for i in eachindex(pred)
        pred[i] = SVector{params.K,Float64}(pred_[i])
    end
    # eval init
    if size(Y_eval, 1) > 0
        pred_eval = zeros(SVector{params.K,Float64}, size(X_eval,1))
        pred_eval_ = predict(model, X_eval)
        for i in eachindex(pred_eval)
            pred_eval[i] = SVector{params.K,Float64}(pred_eval_[i])
        end
    end

    X_size = size(X)
    ùëñ_ = collect(1:X_size[1])
    ùëó_ = collect(1:X_size[2])

    # initialize gradients and weights
    Œ¥, Œ¥¬≤ = zeros(SVector{params.K, Float64}, X_size[1]), zeros(SVector{params.K, Float64}, X_size[1])
    ùë§ = zeros(SVector{1, Float64}, X_size[1]) .+ 1

    edges = get_edges(X, params.nbins)
    X_bin = binarize(X, edges)

    # initialize train nodes
    train_nodes = Vector{TrainNode{params.K, Float64, Int64}}(undef, 2^params.max_depth-1)
    for node in 1:2^params.max_depth-1
        train_nodes[node] = TrainNode(0, SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{1, Float64}(fill(-Inf, 1)), -Inf, [0], [0])
    end

    # initializde node splits info and tracks - colsample size (ùëó)
    splits = Vector{SplitInfo{params.K, Float64, Int64}}(undef, X_size[2])
    hist_Œ¥ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_Œ¥¬≤ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_ùë§ = Vector{Vector{SVector{1, Float64}}}(undef, X_size[2])
    for feat in ùëó_
        splits[feat] = SplitInfo{params.K, Float64, Int}(-Inf, SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), -Inf, -Inf, 0, feat, 0.0)
        hist_Œ¥[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_Œ¥¬≤[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_ùë§[feat] = zeros(SVector{1, Float64}, length(edges[feat]))
    end

    # initialize metric
    if params.metric != :none
        metric_track = model.metric
        metric_best = model.metric
        iter_since_best = 0
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ùëñ = ùëñ_[sample(ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace=false, ordered=true)]
        ùëó = ùëó_[sample(ùëó_, ceil(Int, params.colsample * X_size[2]), replace=false, ordered=true)]

        # reset gain to -Inf
        for feat in ùëó_
            splits[feat].gain = -Inf
        end

        # get gradients
        update_grads!(params.loss, params.Œ±, pred, Y, Œ¥, Œ¥¬≤, ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(Œ¥[ùëñ]), sum(Œ¥¬≤[ùëñ]), sum(ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(Œ¥, Œ¥¬≤, ùë§, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§, params, train_nodes, splits, edges, X_bin)

        # update push tree to model
        push!(model.trees, tree)

        # get update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end

        # callback function
        if params.metric != :none

            if size(Y_eval, 1) > 0
                metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Œ±)
            else
                metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Œ±)
            end

            if metric_track.metric < metric_best.metric
                metric_best.metric .=  metric_track.metric
                metric_best.iter .=  i
            else
                iter_since_best += 1
            end

            if mod(i, print_every_n) == 0 && verbosity > 0
                display(string("iter:", i, ", eval: ", metric_track.metric))
            end
            iter_since_best >= early_stopping_rounds ? break : nothing
        end
    end #end of nrounds

    if params.metric != :none
        model.metric.iter .= metric_best.iter
        model.metric.metric .= metric_best.metric
    end
    return model
end




# grow_gbtree
function grow_gbtree_MLJ(X::AbstractMatrix{R}, Y::AbstractVector{S}, params::Union{EvoTreeRegressor,EvoTreeCount,EvoTreeClassifier,EvoTreeGaussian}; verbosity=1) where {R<:Real, S<:Real}

    seed!(params.seed)

    Œº = ones(params.K)
    Œº .*= mean(Y)
    if typeof(params.loss) == Logistic
        Œº .= logit.(Œº)
    elseif typeof(params.loss) == Poisson
        Œº .= log.(Œº)
    elseif typeof(params.loss) == Softmax
        Œº .*= 0.0
    elseif typeof(params.loss) == Gaussian
        Œº = SVector{2}([mean(Y), log(var(Y))])
    end

    # initialize preds
    pred = zeros(SVector{params.K,Float64}, size(X,1))
    for i in eachindex(pred)
        pred[i] += Œº
    end

    # bias = Tree([TreeNode(SVector{1, Float64}(Œº))])
    bias = Tree([TreeNode(SVector{params.K,Float64}(Œº))])
    gbtree = GBTree([bias], params, Metric())

    X_size = size(X)
    ùëñ_ = collect(1:X_size[1])
    ùëó_ = collect(1:X_size[2])

    # initialize gradients and weights
    Œ¥, Œ¥¬≤ = zeros(SVector{params.K, Float64}, X_size[1]), zeros(SVector{params.K, Float64}, X_size[1])
    ùë§ = zeros(SVector{1, Float64}, X_size[1]) .+ 1

    edges = get_edges(X, params.nbins)
    X_bin = binarize(X, edges)

    # initialize train nodes
    train_nodes = Vector{TrainNode{params.K, Float64, Int64}}(undef, 2^params.max_depth-1)
    for node in 1:2^params.max_depth-1
        train_nodes[node] = TrainNode(0, SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{params.K, Float64}(fill(-Inf, params.K)), SVector{1, Float64}(fill(-Inf, 1)), -Inf, [0], [0])
    end

    # initializde node splits info and tracks - colsample size (ùëó)
    splits = Vector{SplitInfo{params.K, Float64, Int64}}(undef, X_size[2])
    hist_Œ¥ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_Œ¥¬≤ = Vector{Vector{SVector{params.K, Float64}}}(undef, X_size[2])
    hist_ùë§ = Vector{Vector{SVector{1, Float64}}}(undef, X_size[2])
    for feat in ùëó_
        splits[feat] = SplitInfo{params.K, Float64, Int}(-Inf, SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), SVector{params.K, Float64}(zeros(params.K)), SVector{params.K, Float64}(zeros(params.K)), SVector{1, Float64}(zeros(1)), -Inf, -Inf, 0, feat, 0.0)
        hist_Œ¥[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_Œ¥¬≤[feat] = zeros(SVector{params.K, Float64}, length(edges[feat]))
        hist_ùë§[feat] = zeros(SVector{1, Float64}, length(edges[feat]))
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ùëñ = ùëñ_[sample(ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace=false, ordered=true)]
        ùëó = ùëó_[sample(ùëó_, ceil(Int, params.colsample * X_size[2]), replace=false, ordered=true)]

        # reset gain to -Inf
        for feat in ùëó_
            splits[feat].gain = -Inf
        end

        # get gradients
        update_grads!(params.loss, params.Œ±, pred, Y, Œ¥, Œ¥¬≤, ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(Œ¥[ùëñ]), sum(Œ¥¬≤[ùëñ]), sum(ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(Œ¥, Œ¥¬≤, ùë§, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§, params, train_nodes, splits, edges, X_bin)
        # push new tree to model
        push!(gbtree.trees, tree)

        # get update predictions
        predict!(pred, tree, X)

    end #end of nrounds

    cache = (params=deepcopy(params), X=X, Y=Y, pred=pred, ùëñ_=ùëñ_, ùëó_=ùëó_, Œ¥=Œ¥, Œ¥¬≤=Œ¥¬≤, ùë§=ùë§, edges=edges, X_bin=X_bin,
        train_nodes=train_nodes, splits=splits, hist_Œ¥=hist_Œ¥, hist_Œ¥¬≤=hist_Œ¥¬≤, hist_ùë§=hist_ùë§)
    # cache = (deepcopy(params), X, Y, pred, ùëñ_, ùëó_, Œ¥, Œ¥¬≤, ùë§, edges, X_bin, train_nodes, splits, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§)
    return gbtree, cache
end

# continue training for MLJ - continue training from same dataset - all preprocessed elements passed as cache
function grow_gbtree_MLJ!(model::GBTree, cache; verbosity=1)

    params = model.params

    # initialize predictions
    # cache_params, X, Y, pred, ùëñ_, ùëó_, Œ¥, Œ¥¬≤, ùë§, edges, X_bin, train_nodes, splits, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§ = cache
    train_nodes = cache.train_nodes
    splits = cache.splits

    X_size = size(cache.X_bin)
    Œ¥nrounds = params.nrounds - cache.params.nrounds
    # println("MLJ! Œ¥nrounds: ", Œ¥nrounds)

    # loop over nrounds
    for i in 1:Œ¥nrounds

        # select random rows and cols
        ùëñ = cache.ùëñ_[sample(cache.ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace=false, ordered=true)]
        ùëó = cache.ùëó_[sample(cache.ùëó_, ceil(Int, params.colsample * X_size[2]), replace=false, ordered=true)]

        # reset gain to -Inf
        for feat in cache.ùëó_
            splits[feat].gain = -Inf
        end

        # get gradients
        update_grads!(params.loss, params.Œ±, cache.pred, cache.Y, cache.Œ¥, cache.Œ¥¬≤, cache.ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(cache.Œ¥[ùëñ]), sum(cache.Œ¥¬≤[ùëñ]), sum(cache.ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(cache.Œ¥, cache.Œ¥¬≤, cache.ùë§, cache.hist_Œ¥, cache.hist_Œ¥¬≤, cache.hist_ùë§, params, train_nodes, splits, cache.edges, cache.X_bin)

        # update push tree to model
        push!(model.trees, tree)

        # get update predictions
        predict!(cache.pred, tree, cache.X)

    end #end of nrounds


    cache.params.nrounds = params.nrounds
    # cache = (deepcopy(params), X, Y, pred, ùëñ_, ùëó_, Œ¥, Œ¥¬≤, ùë§, edges, X_bin, train_nodes, splits, hist_Œ¥, hist_Œ¥¬≤, hist_ùë§)

    # return model, cache
    return model
end
